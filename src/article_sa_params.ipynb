{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92a4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bagging import create_models, create_bags, evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from BaggingSA import BaggingSA\n",
    "from typing import Literal, Tuple\n",
    "from Bagging import predict\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ad331",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "k_cross = 5\n",
    "fitness_accuracy_diversity_ratios = [.25, .5, .75] \n",
    "feature_mutation_chances = [.1, .2, .3]\n",
    "# datasets = ['wine', 'breast_cancer', 'pima', 'digits']\n",
    "datasets = ['digits', 'pima']\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7538a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_dataset(dataset_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if dataset_name == 'digits':\n",
    "        data = sklearn.datasets.load_digits()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "    elif dataset_name == 'wine':\n",
    "        data = sklearn.datasets.load_wine()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "    \n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = sklearn.datasets.load_breast_cancer()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "    elif dataset_name == 'pima':\n",
    "        data = pd.read_csv(\"./../datasets/pima.csv\")\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f1088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2025-04-15 12:11:42.775218\n",
      "[Dataset: digits, FADR: 0.25, FMC: 0.1, k: 0]\n",
      "Iteration: 100, T: 1.22, Best fitness: 0.9543\n",
      "Iteration: 200, T: 0.74, Best fitness: 0.9635\n",
      "Iteration: 300, T: 0.45, Best fitness: 0.9724\n",
      "Iteration: 400, T: 0.27, Best fitness: 0.9724\n",
      "Iteration: 500, T: 0.16, Best fitness: 0.9740\n",
      "Iteration: 600, T: 0.10, Best fitness: 0.9740\n",
      "Iteration: 700, T: 0.06, Best fitness: 0.9740\n",
      "Iteration: 800, T: 0.04, Best fitness: 0.9740\n",
      "Iteration: 900, T: 0.02, Best fitness: 0.9740\n",
      "Iteration: 1000, T: 0.01, Best fitness: 0.9740\n",
      "Iteration: 1100, T: 0.01, Best fitness: 0.9740\n",
      "Iteration: 1200, T: 0.00, Best fitness: 0.9740\n",
      "Iteration: 1300, T: 0.00, Best fitness: 0.9740\n",
      "Iteration: 1400, T: 0.00, Best fitness: 0.9811\n",
      "Iteration: 1500, T: 0.00, Best fitness: 0.9844\n",
      "Iteration: 1600, T: 0.00, Best fitness: 0.9849\n",
      "Iteration: 1700, T: 0.00, Best fitness: 0.9849\n",
      "Iteration: 1800, T: 0.00, Best fitness: 0.9849\n",
      "Iteration: 1900, T: 0.00, Best fitness: 0.9849\n",
      "Iteration: 2000, T: 0.00, Best fitness: 0.9849\n",
      "    Accuracy: 0.8556\n",
      "[Dataset: digits, FADR: 0.25, FMC: 0.1, k: 1]\n",
      "Iteration: 100, T: 1.22, Best fitness: 0.9745\n",
      "Iteration: 200, T: 0.74, Best fitness: 0.9745\n",
      "Iteration: 300, T: 0.45, Best fitness: 0.9745\n",
      "Iteration: 400, T: 0.27, Best fitness: 0.9745\n",
      "Iteration: 500, T: 0.16, Best fitness: 0.9745\n",
      "Iteration: 600, T: 0.10, Best fitness: 0.9745\n",
      "Iteration: 700, T: 0.06, Best fitness: 0.9745\n",
      "Iteration: 800, T: 0.04, Best fitness: 0.9745\n",
      "Iteration: 900, T: 0.02, Best fitness: 0.9745\n",
      "Iteration: 1000, T: 0.01, Best fitness: 0.9745\n",
      "Iteration: 1100, T: 0.01, Best fitness: 0.9745\n",
      "Iteration: 1200, T: 0.00, Best fitness: 0.9745\n",
      "Iteration: 1300, T: 0.00, Best fitness: 0.9758\n",
      "Iteration: 1400, T: 0.00, Best fitness: 0.9791\n",
      "Iteration: 1500, T: 0.00, Best fitness: 0.9799\n",
      "Iteration: 1600, T: 0.00, Best fitness: 0.9799\n",
      "Iteration: 1700, T: 0.00, Best fitness: 0.9799\n",
      "Iteration: 1800, T: 0.00, Best fitness: 0.9805\n",
      "Iteration: 1900, T: 0.00, Best fitness: 0.9805\n",
      "Iteration: 2000, T: 0.00, Best fitness: 0.9805\n",
      "    Accuracy: 0.8944\n",
      "[Dataset: digits, FADR: 0.25, FMC: 0.1, k: 2]\n",
      "Iteration: 100, T: 1.22, Best fitness: 0.9443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m acc_fitness_difference = \u001b[32m0.0\u001b[39m\n\u001b[32m     43\u001b[39m fit_acc_sum = {}\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m bagging_sa, accuracy, fitness = \u001b[43mevaluate_bagging_sa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfadr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m X = np.array(\u001b[38;5;28mlist\u001b[39m(fit_acc_sum.keys()))\n\u001b[32m     48\u001b[39m Y = np.array([np.mean(fit_acc_sum[x]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mevaluate_bagging_sa\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, fitness_accuracy_diversity_ratio, feature_mutation_chance)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_bagging_sa\u001b[39m(X_train, y_train, X_test, y_test, fitness_accuracy_diversity_ratio, feature_mutation_chance) -> Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]: \n\u001b[32m      2\u001b[39m     bagging_sa = BaggingSA(X=X_train, y=y_train,\n\u001b[32m      3\u001b[39m                             T0=\u001b[32m2.0\u001b[39m, cooling_method=\u001b[33m'\u001b[39m\u001b[33mgeometric\u001b[39m\u001b[33m'\u001b[39m, alpha=\u001b[32m0.995\u001b[39m, max_iterations=\u001b[32m2000\u001b[39m, n_trees=\u001b[32m10\u001b[39m,\n\u001b[32m      4\u001b[39m                             fitness_accuracy_diversity_ratio=fitness_accuracy_diversity_ratio,\n\u001b[32m      5\u001b[39m                             feature_mutation_chance=feature_mutation_chance, test_split_amount=\u001b[32m20\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     models, fitness = \u001b[43mbagging_sa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor_fun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfun_monitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_fitness\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_for_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_for_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     accuracy = evaluate(X=X_test, y=y_test, models=models)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m bagging_sa, accuracy, fitness\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\BaggingSA.py:144\u001b[39m, in \u001b[36mBaggingSA.run\u001b[39m\u001b[34m(self, X_for_test, y_for_test, monitor_fun, get_fitness)\u001b[39m\n\u001b[32m    142\u001b[39m new_bags = \u001b[38;5;28mself\u001b[39m.get_neighbors(bags)\n\u001b[32m    143\u001b[39m models = create_models(\u001b[38;5;28mself\u001b[39m.X_train, \u001b[38;5;28mself\u001b[39m.y_train, new_bags)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m new_fitness = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m accuracy = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_for_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y_for_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\BaggingSA.py:62\u001b[39m, in \u001b[36mBaggingSA.calculate_fitness\u001b[39m\u001b[34m(self, models)\u001b[39m\n\u001b[32m     59\u001b[39m     sub_X, sub_y = sub_groups_X_test[i], sub_groups_y_test[i]\n\u001b[32m     61\u001b[39m     acc_sum += evaluate(X=sub_X, y=sub_y, models=models)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     qstat_sum += \u001b[43mq_statistic_for_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m accuracy = acc_sum / \u001b[38;5;28mself\u001b[39m.test_split_amount\n\u001b[32m     66\u001b[39m qstat = qstat_sum / \u001b[38;5;28mself\u001b[39m.test_split_amount\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\Bagging.py:103\u001b[39m, in \u001b[36mq_statistic_for_ensemble\u001b[39m\u001b[34m(X, y, models)\u001b[39m\n\u001b[32m    100\u001b[39m correct_matrix = np.empty((n_models, n_samples), dtype=\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     correct_matrix[idx] = preds == y\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Step 2: Compute pairwise Q-statistics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:531\u001b[39m, in \u001b[36mBaseDecisionTree.predict\u001b[39m\u001b[34m(self, X, check_input)\u001b[39m\n\u001b[32m    529\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    530\u001b[39m X = \u001b[38;5;28mself\u001b[39m._validate_X_predict(X, check_input)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m n_samples = X.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    534\u001b[39m \u001b[38;5;66;03m# Classification\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_bagging_sa(X_train, y_train, X_test, y_test, fitness_accuracy_diversity_ratio, feature_mutation_chance) -> Tuple[float, int, int]: \n",
    "    bagging_sa = BaggingSA(X=X_train, y=y_train,\n",
    "                            T0=2.0, cooling_method='geometric', alpha=0.995, max_iterations=2000, n_trees=10,\n",
    "                            fitness_accuracy_diversity_ratio=fitness_accuracy_diversity_ratio,\n",
    "                            feature_mutation_chance=feature_mutation_chance, test_split_amount=20)\n",
    "    models, fitness = bagging_sa.run(monitor_fun=fun_monitor, get_fitness=True, X_for_test=X_test, y_for_test=y_test)\n",
    "    accuracy = evaluate(X=X_test, y=y_test, models=models)\n",
    "    return bagging_sa, accuracy, fitness\n",
    "    \n",
    "def fun_monitor(iteration, T, best_fitness, fitness, new_fitness, accuracy):\n",
    "    global fit_acc_sum, acc_fitness_difference\n",
    "    \n",
    "    acc_fitness_difference += abs(accuracy - fitness)\n",
    "    \n",
    "    if new_fitness not in fit_acc_sum:\n",
    "        fit_acc_sum[new_fitness] = [accuracy]\n",
    "    else:\n",
    "        fit_acc_sum[new_fitness].append(accuracy)\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"    Iteration: {iteration}, T: {T:.2f}, Best fitness: {best_fitness:.4f}\")\n",
    "\n",
    "acc_fitness_difference = 0.0\n",
    "fit_acc_sum = {}\n",
    "result = []\n",
    "print(f\"Start at {pd.Timestamp.now()}\")\n",
    "for dataset in datasets:\n",
    "    X, y = get_dataset(dataset)       \n",
    "    sub_groups_X = np.array_split(np.array(X), k_cross)\n",
    "    sub_groups_y = np.array_split(np.array(y), k_cross) \n",
    "         \n",
    "    for fadr in fitness_accuracy_diversity_ratios:\n",
    "        for fmc in feature_mutation_chances:\n",
    "            for k in range(k_cross):\n",
    "                print(f\"[Dataset: {dataset}, FADR: {fadr}, FMC: {fmc}, k: {k}]\")\n",
    "                \n",
    "                X_train = np.concatenate(sub_groups_X[:k] + sub_groups_X[k+1:])\n",
    "                y_train = np.concatenate(sub_groups_y[:k] + sub_groups_y[k+1:])\n",
    "                X_test = sub_groups_X[k]\n",
    "                y_test = sub_groups_y[k]\n",
    "                \n",
    "                acc_fitness_difference = 0.0\n",
    "                fit_acc_sum = {}\n",
    "                \n",
    "                bagging_sa, accuracy, fitness = evaluate_bagging_sa(X_train, y_train, X_test, y_test, fadr, fmc)\n",
    "                \n",
    "                X = np.array(list(fit_acc_sum.keys()))\n",
    "                Y = np.array([np.mean(fit_acc_sum[x]) for x in X])\n",
    "                correlation = np.corrcoef(X, Y)[0, 1]\n",
    "                \n",
    "                acc_fitness_difference /= bagging_sa.max_iterations\n",
    "                \n",
    "                result.append([dataset, k, fadr, fmc, accuracy, correlation, fitness, acc_fitness_difference])\n",
    "                \n",
    "                df = pd.DataFrame(result, columns=[\"dataset\", \"kCrossIndex\", \"fadr\", \"fmc\", \"accuracy\", \"correlation\", \"fitness\", \"accFitnessDifference\"])\n",
    "                df.to_csv(\"./../res/bagging_sa_params.csv\", index=False)\n",
    "                print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            \n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550f5e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset  fadr  fmc  accuracyMean  accuracyStd  correlationMean  \\\n",
      "0  digits  0.25  0.1         0.875     0.027499         0.261226   \n",
      "\n",
      "   correlationStd  fitnessMean  fitnessStd  accFitnessDifferenceMean  \\\n",
      "0         0.16773     0.982706    0.003168                  0.121549   \n",
      "\n",
      "   accFitnessDifferenceStd  \n",
      "0                 0.046606  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./../res/bagging_sa_params.csv\")\n",
    "df_agg = df.groupby([\"dataset\", \"fadr\", \"fmc\"]).agg(\n",
    "    dataset=(\"dataset\", \"first\"),\n",
    "    fadr=(\"fadr\", \"first\"),\n",
    "    fmc=(\"fmc\", \"first\"),\n",
    "    accuracyMean=(\"accuracy\", \"mean\"),\n",
    "    accuracyStd=(\"accuracy\", \"std\"),\n",
    "    correlationMean=(\"correlation\", \"mean\"),\n",
    "    correlationStd=(\"correlation\", \"std\"),\n",
    "    fitnessMean=(\"fitness\", \"mean\"),\n",
    "    fitnessStd=(\"fitness\", \"std\"),\n",
    "    accFitnessDifferenceMean=(\"accFitnessDifference\", \"mean\"),\n",
    "    accFitnessDifferenceStd=(\"accFitnessDifference\", \"std\"),\n",
    ").reset_index(drop=True)\n",
    "df_agg.to_csv(\"./../res/bagging_sa_params_aggregated.csv\", index=False)\n",
    "\n",
    "print(df_agg.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
