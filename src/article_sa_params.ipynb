{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92a4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bagging import create_models, create_bags, evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from BaggingSA import BaggingSA\n",
    "from typing import Literal, Tuple\n",
    "from Bagging import predict\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ad331",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "k_cross = 10\n",
    "fitness_accuracy_disagreement_ratios = [.6, .7, .8, .9] \n",
    "feature_mutation_chances = [.1,.2,.3,.4]\n",
    "# datasets = ['wine', 'breast_cancer', 'pima', 'digits']\n",
    "datasets = ['digits', 'pima']\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7538a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_dataset(dataset_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if dataset_name == 'digits':\n",
    "        data = sklearn.datasets.load_digits()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "    elif dataset_name == 'wine':\n",
    "        data = sklearn.datasets.load_wine()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "    \n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = sklearn.datasets.load_breast_cancer()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "    elif dataset_name == 'pima':\n",
    "        data = pd.read_csv(\"./../datasets/pima.csv\")\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2025-04-14 13:04:15.171928\n",
      "[Dataset: abalone, FADR: 0.6, FMC: 0.1, k: 0]\n",
      "Iteration: 100, T: 3.70, Best fitness: 0.5672\n",
      "Iteration: 200, T: 1.35, Best fitness: 0.5672\n",
      "Iteration: 300, T: 0.50, Best fitness: 0.5672\n",
      "Iteration: 400, T: 0.18, Best fitness: 0.5672\n",
      "Iteration: 500, T: 0.07, Best fitness: 0.5672\n",
      "Iteration: 600, T: 0.02, Best fitness: 0.5672\n",
      "Iteration: 700, T: 0.01, Best fitness: 0.5736\n",
      "Iteration: 800, T: 0.00, Best fitness: 0.5736\n",
      "Iteration: 900, T: 0.00, Best fitness: 0.5736\n",
      "Iteration: 1000, T: 0.00, Best fitness: 0.5736\n",
      "Iteration: 1100, T: 0.00, Best fitness: 0.5858\n",
      "Iteration: 1200, T: 0.00, Best fitness: 0.5858\n",
      "Iteration: 1300, T: 0.00, Best fitness: 0.5878\n",
      "Iteration: 1400, T: 0.00, Best fitness: 0.5878\n",
      "Iteration: 1500, T: 0.00, Best fitness: 0.5878\n",
      "Iteration: 1600, T: 0.00, Best fitness: 0.5878\n",
      "Iteration: 1700, T: 0.00, Best fitness: 0.5878\n",
      "Iteration: 1800, T: 0.00, Best fitness: 0.5900\n",
      "Iteration: 1900, T: 0.00, Best fitness: 0.5932\n",
      "Iteration: 2000, T: 0.00, Best fitness: 0.5932\n",
      "    Accuracy: 0.1507\n",
      "[Dataset: abalone, FADR: 0.6, FMC: 0.1, k: 1]\n",
      "Iteration: 100, T: 3.70, Best fitness: 0.5605\n",
      "Iteration: 200, T: 1.35, Best fitness: 0.5669\n",
      "Iteration: 300, T: 0.50, Best fitness: 0.5669\n",
      "Iteration: 400, T: 0.18, Best fitness: 0.5669\n",
      "Iteration: 500, T: 0.07, Best fitness: 0.5669\n",
      "Iteration: 600, T: 0.02, Best fitness: 0.5669\n",
      "Iteration: 700, T: 0.01, Best fitness: 0.5669\n",
      "Iteration: 800, T: 0.00, Best fitness: 0.5678\n",
      "Iteration: 900, T: 0.00, Best fitness: 0.5678\n",
      "Iteration: 1000, T: 0.00, Best fitness: 0.5678\n",
      "Iteration: 1100, T: 0.00, Best fitness: 0.5786\n",
      "Iteration: 1200, T: 0.00, Best fitness: 0.5852\n",
      "Iteration: 1300, T: 0.00, Best fitness: 0.5852\n",
      "Iteration: 1400, T: 0.00, Best fitness: 0.5852\n",
      "Iteration: 1500, T: 0.00, Best fitness: 0.5852\n",
      "Iteration: 1600, T: 0.00, Best fitness: 0.5852\n",
      "Iteration: 1700, T: 0.00, Best fitness: 0.5852\n",
      "Iteration: 1800, T: 0.00, Best fitness: 0.5852\n",
      "Iteration: 1900, T: 0.00, Best fitness: 0.5852\n",
      "Iteration: 2000, T: 0.00, Best fitness: 0.5852\n",
      "    Accuracy: 0.0837\n",
      "[Dataset: abalone, FADR: 0.6, FMC: 0.1, k: 2]\n",
      "Iteration: 100, T: 3.70, Best fitness: 0.5571\n",
      "Iteration: 200, T: 1.35, Best fitness: 0.5571\n",
      "Iteration: 300, T: 0.50, Best fitness: 0.5571\n",
      "Iteration: 400, T: 0.18, Best fitness: 0.5571\n",
      "Iteration: 500, T: 0.07, Best fitness: 0.5571\n",
      "Iteration: 600, T: 0.02, Best fitness: 0.5571\n",
      "Iteration: 700, T: 0.01, Best fitness: 0.5571\n",
      "Iteration: 800, T: 0.00, Best fitness: 0.5571\n",
      "Iteration: 900, T: 0.00, Best fitness: 0.5571\n",
      "Iteration: 1000, T: 0.00, Best fitness: 0.5571\n",
      "Iteration: 1100, T: 0.00, Best fitness: 0.5581\n",
      "Iteration: 1200, T: 0.00, Best fitness: 0.5581\n",
      "Iteration: 1300, T: 0.00, Best fitness: 0.5581\n",
      "Iteration: 1400, T: 0.00, Best fitness: 0.5583\n",
      "Iteration: 1500, T: 0.00, Best fitness: 0.5583\n",
      "Iteration: 1600, T: 0.00, Best fitness: 0.5598\n",
      "Iteration: 1700, T: 0.00, Best fitness: 0.5598\n",
      "Iteration: 1800, T: 0.00, Best fitness: 0.5606\n",
      "Iteration: 1900, T: 0.00, Best fitness: 0.5616\n",
      "Iteration: 2000, T: 0.00, Best fitness: 0.5616\n",
      "    Accuracy: 0.3397\n",
      "[Dataset: abalone, FADR: 0.6, FMC: 0.1, k: 3]\n",
      "Iteration: 100, T: 3.70, Best fitness: 0.5430\n",
      "Iteration: 200, T: 1.35, Best fitness: 0.5430\n",
      "Iteration: 300, T: 0.50, Best fitness: 0.5430\n",
      "Iteration: 400, T: 0.18, Best fitness: 0.5430\n",
      "Iteration: 500, T: 0.07, Best fitness: 0.5430\n",
      "Iteration: 600, T: 0.02, Best fitness: 0.5430\n",
      "Iteration: 700, T: 0.01, Best fitness: 0.5430\n",
      "Iteration: 800, T: 0.00, Best fitness: 0.5497\n",
      "Iteration: 900, T: 0.00, Best fitness: 0.5497\n",
      "Iteration: 1000, T: 0.00, Best fitness: 0.5535\n",
      "Iteration: 1100, T: 0.00, Best fitness: 0.5535\n",
      "Iteration: 1200, T: 0.00, Best fitness: 0.5578\n",
      "Iteration: 1300, T: 0.00, Best fitness: 0.5578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m X_test = sub_groups_X[k]\n\u001b[32m     32\u001b[39m y_test = sub_groups_y[k]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m accuracy = \u001b[43mevaluate_bagging_sa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfadr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m result.append([dataset, k, fadr, fmc, accuracy])\n\u001b[32m     38\u001b[39m df = pd.DataFrame(result, columns=[\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkCrossIndex\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfadr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfmc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mevaluate_bagging_sa\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, fitness_accuracy_disagreement_ratio, feature_mutation_chance)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_bagging_sa\u001b[39m(X_train, y_train, X_test, y_test, fitness_accuracy_disagreement_ratio, feature_mutation_chance) -> Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]: \n\u001b[32m      2\u001b[39m     bagging_sa = BaggingSA(X=X_train, y=y_train,\n\u001b[32m      3\u001b[39m                             T0=\u001b[32m10\u001b[39m, cooling_method=\u001b[33m'\u001b[39m\u001b[33mgeometric\u001b[39m\u001b[33m'\u001b[39m, alpha=\u001b[32m0.99\u001b[39m, max_iterations=\u001b[32m2000\u001b[39m, n_trees=\u001b[32m10\u001b[39m,\n\u001b[32m      4\u001b[39m                             fitness_accuracy_disagreement_ratio=fitness_accuracy_disagreement_ratio,\n\u001b[32m      5\u001b[39m                             feature_mutation_chance=feature_mutation_chance, test_split_amount=\u001b[32m20\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     models = \u001b[43mbagging_sa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor_fun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfun_monitor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     accuracy = evaluate(X=X_test, y=y_test, models=models)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\BaggingSA.py:130\u001b[39m, in \u001b[36mBaggingSA.run\u001b[39m\u001b[34m(self, X_for_test, y_for_test, monitor_fun)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m T > \u001b[32m1e-10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m iteration <= \u001b[38;5;28mself\u001b[39m.max_iterations:\n\u001b[32m    129\u001b[39m     new_bags = \u001b[38;5;28mself\u001b[39m.get_neighbors(bags)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     models = \u001b[43mcreate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_bags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     new_fitness = \u001b[38;5;28mself\u001b[39m.calculate_fitness(models)\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m X_for_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y_for_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\Bagging.py:76\u001b[39m, in \u001b[36mcreate_models\u001b[39m\u001b[34m(X, y, bags)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_models\u001b[39m(X, y, bags: List[Bag]) -> List[BaggingModel]:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     models = [\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m bag \u001b[38;5;129;01min\u001b[39;00m bags]\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\Bagging.py:72\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(X, y, bag)\u001b[39m\n\u001b[32m     70\u001b[39m X_mapped, y_mapped = bag.get_mapped_data(X, y)\n\u001b[32m     71\u001b[39m model = DecisionTreeClassifier()\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_mapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaggingModel(model, bag)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1024\u001b[39m, in \u001b[36mDecisionTreeClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    995\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[32m    996\u001b[39m \n\u001b[32m    997\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1021\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_bagging_sa(X_train, y_train, X_test, y_test, fitness_accuracy_disagreement_ratio, feature_mutation_chance) -> Tuple[float, int, int]: \n",
    "    bagging_sa = BaggingSA(X=X_train, y=y_train,\n",
    "                            T0=1.0, cooling_method='geometric', alpha=0.995, max_iterations=2000, n_trees=10,\n",
    "                            fitness_accuracy_disagreement_ratio=fitness_accuracy_disagreement_ratio,\n",
    "                            feature_mutation_chance=feature_mutation_chance, test_split_amount=20)\n",
    "    models = bagging_sa.run(monitor_fun=fun_monitor)\n",
    "    accuracy = evaluate(X=X_test, y=y_test, models=models)\n",
    "    return accuracy\n",
    "    \n",
    "def fun_monitor(iteration, T, best_fitness, fitness, new_fitness, accuracy):\n",
    "    global fit_acc_sum\n",
    "    \n",
    "    if new_fitness not in fit_acc_sum:\n",
    "        fit_acc_sum[new_fitness] = [accuracy]\n",
    "    else:\n",
    "        fit_acc_sum[new_fitness].append(accuracy)\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"Iteration: {iteration}, T: {T:.2f}, Best fitness: {best_fitness:.4f}\")\n",
    "\n",
    "\n",
    "fit_acc_sum = {}\n",
    "result = []\n",
    "print(f\"Start at {pd.Timestamp.now()}\")\n",
    "for dataset in datasets:\n",
    "    X, y = get_dataset(dataset)       \n",
    "    sub_groups_X = np.array_split(np.array(X), k_cross)\n",
    "    sub_groups_y = np.array_split(np.array(y), k_cross) \n",
    "         \n",
    "    for fadr in fitness_accuracy_disagreement_ratios:\n",
    "        for fmc in feature_mutation_chances:\n",
    "            for k in range(k_cross):\n",
    "                print(f\"[Dataset: {dataset}, FADR: {fadr}, FMC: {fmc}, k: {k}]\")\n",
    "                \n",
    "                X_train = np.concatenate(sub_groups_X[:k] + sub_groups_X[k+1:])\n",
    "                y_train = np.concatenate(sub_groups_y[:k] + sub_groups_y[k+1:])\n",
    "                X_test = sub_groups_X[k]\n",
    "                y_test = sub_groups_y[k]\n",
    "                \n",
    "                fit_acc_sum = {}\n",
    "                accuracy = evaluate_bagging_sa(X_train, y_train, X_test, y_test, fadr, fmc)\n",
    "                \n",
    "                X = np.array(list(fit_acc_sum.keys()))\n",
    "                Y = np.array([np.mean(fit_acc_sum[x]) for x in X])\n",
    "                correlation = np.corrcoef(X, Y)[0, 1]\n",
    "                \n",
    "                result.append([dataset, k, fadr, fmc, accuracy, correlation])\n",
    "                \n",
    "                df = pd.DataFrame(result, columns=[\"dataset\", \"kCrossIndex\", \"fadr\", \"fmc\", \"accuracy\", \"correlation\"])\n",
    "                df.to_csv(\"./../res_article/bagging_sa_params.csv\", index=False)\n",
    "                print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "                \n",
    "dg_aggregated = df.groupby([\"dataset\", \"fadr\", \"fmc\"]).agg(\n",
    "    accuracyMean=(\"accuracy\", \"mean\"),\n",
    "    accuracyStd=(\"accuracy\", \"std\"),\n",
    "    correlationMean=(\"correlation\", \"mean\"),\n",
    "    correlationStd=(\"correlation\", \"std\"),\n",
    "    accuracyMax=(\"accuracy\", \"max\"),\n",
    "    accuracyMin=(\"accuracy\", \"min\"),\n",
    "    correlationMax=(\"correlation\", \"max\"),\n",
    "    correlationMin=(\"correlation\", \"min\"),\n",
    ").reset_index()\n",
    "dg_aggregated.to_csv(\"./../res_article/bagging_sa_params_aggregated.csv\", index=False)\n",
    "print(f\"End at {pd.Timestamp.now()}\")\n",
    "            \n",
    "                \n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
