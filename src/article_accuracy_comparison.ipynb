{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92a4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bagging import create_models, create_bags, evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from BaggingSA import BaggingSA\n",
    "from typing import Literal, Tuple\n",
    "from Bagging import predict\n",
    "import sklearn\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08ad331",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "k_cross = 10\n",
    "n_trees = [10, 20, 30, 40, 50]\n",
    "datasets = ['digits', 'wine', 'breast_cancer', 'pima']\n",
    "\n",
    "bagging_sa_params = {\n",
    "    'wine' : {\n",
    "        'T0': 2,\n",
    "        'cooling_method': 'geometric',\n",
    "        'alpha': 0.995,\n",
    "        'max_iterations': 2000,\n",
    "        'fitness_accuracy_diversity_ratio': 0.75,\n",
    "        'feature_mutation_chance': 0.2,\n",
    "        'test_split_amount': 5        \n",
    "    },\n",
    "    'breast_cancer' : {\n",
    "        'T0': 2,\n",
    "        'cooling_method': 'geometric',\n",
    "        'alpha': 0.995,\n",
    "        'max_iterations': 2000,\n",
    "        'fitness_accuracy_diversity_ratio': 0.5,\n",
    "        'feature_mutation_chance': 0.3,\n",
    "        'test_split_amount': 5        \n",
    "    },\n",
    "    'pima' : {\n",
    "        'T0': 2,\n",
    "        'cooling_method': 'geometric',\n",
    "        'alpha': 0.995,\n",
    "        'max_iterations': 2000,\n",
    "        'fitness_accuracy_diversity_ratio': 0.75,\n",
    "        'feature_mutation_chance': 0.3,\n",
    "        'test_split_amount': 5        \n",
    "    },\n",
    "    \n",
    "    'digits' : {\n",
    "        'T0': 2,\n",
    "        'cooling_method': 'geometric',\n",
    "        'alpha': 0.995,\n",
    "        'max_iterations': 2000,\n",
    "        'fitness_accuracy_diversity_ratio': 0.75,\n",
    "        'feature_mutation_chance': 0.3,\n",
    "        'test_split_amount': 5        \n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7538a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if dataset_name == 'digits':\n",
    "        data = sklearn.datasets.load_digits()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "    elif dataset_name == 'wine':\n",
    "        data = sklearn.datasets.load_wine()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "    \n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = sklearn.datasets.load_breast_cancer()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "    elif dataset_name == 'pima':\n",
    "        data = pd.read_csv(\"./../datasets/pima.csv\")\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f1088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2025-04-22 12:38:57.578721\n",
      "    Dataset: digits, n_trees: 10, k: 1/10 >> DT: 0.8833, Bagging: 0.8222, BaggingSA: 0.8722\n",
      "    Dataset: digits, n_trees: 10, k: 2/10 >> DT: 0.8167, Bagging: 0.8944, BaggingSA: 0.9000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m bagging_acc, bagging_correct, bagging_wrong = evaluate_bagging(X_train, y_train, X_test, y_test, n_trees=n_tree)\n\u001b[32m     76\u001b[39m pars = bagging_sa_params[dataset]\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m bagging_sa_acc, bagging_sa_correct, bagging_sa_wrong = \u001b[43mevaluate_bagging_sa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trees\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, n_trees: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tree\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk_cross\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m >> DT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Bagging: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbagging_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, BaggingSA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbagging_sa_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m result.append([\n\u001b[32m     82\u001b[39m     dataset, n_tree, k+\u001b[32m1\u001b[39m, dt_acc, bagging_acc, bagging_sa_acc,\n\u001b[32m     83\u001b[39m     dt_correct, bagging_correct, bagging_sa_correct,\n\u001b[32m     84\u001b[39m     dt_wrong, bagging_wrong, bagging_sa_wrong\n\u001b[32m     85\u001b[39m ])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mevaluate_bagging_sa\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, n_trees, params)\u001b[39m\n\u001b[32m     35\u001b[39m test_split_amount = params[\u001b[33m'\u001b[39m\u001b[33mtest_split_amount\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     36\u001b[39m bagging_sa = BaggingSA(X=X_train, y=y_train,\n\u001b[32m     37\u001b[39m                         T0=T0, cooling_method=cooling_method, alpha=alpha, max_iterations=max_iterations, n_trees=n_trees,\n\u001b[32m     38\u001b[39m                         fitness_accuracy_diversity_ratio=fitness_accuracy_diversity_ratio,\n\u001b[32m     39\u001b[39m                         feature_mutation_chance=feature_mutation_chance, test_split_amount=test_split_amount)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m models = \u001b[43mbagging_sa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m predictions = predict(X=X_test, models=models)\n\u001b[32m     42\u001b[39m accuracy, correct_pred_amount, wrong_pred_amount = get_measures(predictions, y_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\BaggingSA.py:148\u001b[39m, in \u001b[36mBaggingSA.run\u001b[39m\u001b[34m(self, X_for_test, y_for_test, monitor_fun, get_fitness)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m T > \u001b[32m1e-10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m iteration <= \u001b[38;5;28mself\u001b[39m.max_iterations \u001b[38;5;129;01mand\u001b[39;00m best_fitness < \u001b[32m1.0\u001b[39m:\n\u001b[32m    147\u001b[39m     new_bags = \u001b[38;5;28mself\u001b[39m.get_neighbors(bags)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     models = \u001b[43mcreate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_bags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     new_fitness = \u001b[38;5;28mself\u001b[39m.calculate_fitness(models)\n\u001b[32m    151\u001b[39m     accuracy = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\Bagging.py:77\u001b[39m, in \u001b[36mcreate_models\u001b[39m\u001b[34m(X, y, bags)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_models\u001b[39m(X, y, bags: List[Bag]) -> List[BaggingModel]:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     models = [\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m bag \u001b[38;5;129;01min\u001b[39;00m bags]\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\src\\Bagging.py:73\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(X, y, bag)\u001b[39m\n\u001b[32m     71\u001b[39m X_mapped, y_mapped = bag.get_mapped_data(X, y)\n\u001b[32m     72\u001b[39m model = DecisionTreeClassifier(max_depth=\u001b[38;5;28;01mNone\u001b[39;00m, min_samples_split=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_mapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaggingModel(model, bag)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1024\u001b[39m, in \u001b[36mDecisionTreeClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    995\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[32m    996\u001b[39m \n\u001b[32m    997\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1021\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\TMP\\SimulatedAnnealingBagging\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_measures(predictions, y_test):\n",
    "    correct_pred_amount = 0\n",
    "    wrong_pred_amount = 0\n",
    "    accuracy = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == y_test[i]:\n",
    "            correct_pred_amount += 1\n",
    "        else:\n",
    "            wrong_pred_amount += 1\n",
    "    accuracy = correct_pred_amount / (correct_pred_amount + wrong_pred_amount)\n",
    "    return accuracy, correct_pred_amount, wrong_pred_amount\n",
    "    \n",
    "def evaluate_rf(X_train, y_train, X_test, y_test, n_trees: int) -> Tuple[float, int, int]:\n",
    "    model = RandomForestClassifier(n_estimators=n_trees, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy, correct_pred_amount, wrong_pred_amount = get_measures(predictions, y_test)\n",
    "    return accuracy, correct_pred_amount, wrong_pred_amount\n",
    "\n",
    "def evaluate_bagging(X_train, y_train, X_test, y_test, n_trees: int) -> Tuple[float, int, int]:\n",
    "    bags = create_bags(X_train, bags_amount=n_trees)\n",
    "    models = create_models(X=X_train, y=y_train, bags=bags)\n",
    "    predictions = predict(X=X_test, models=models)\n",
    "    accuracy, correct_pred_amount, wrong_pred_amount = get_measures(predictions, y_test)\n",
    "    return accuracy, correct_pred_amount, wrong_pred_amount\n",
    "  \n",
    "    \n",
    "def evaluate_bagging_sa(X_train, y_train, X_test, y_test, n_trees: int, params: dict) -> Tuple[float, int, int]: \n",
    "    T0 = params['T0']\n",
    "    cooling_method = params['cooling_method']\n",
    "    alpha = params['alpha']\n",
    "    max_iterations = params['max_iterations']\n",
    "    fitness_accuracy_diversity_ratio = params['fitness_accuracy_diversity_ratio']\n",
    "    feature_mutation_chance = params['feature_mutation_chance']\n",
    "    test_split_amount = params['test_split_amount']\n",
    "    bagging_sa = BaggingSA(X=X_train, y=y_train,\n",
    "                            T0=T0, cooling_method=cooling_method, alpha=alpha, max_iterations=max_iterations, n_trees=n_trees,\n",
    "                            fitness_accuracy_diversity_ratio=fitness_accuracy_diversity_ratio,\n",
    "                            feature_mutation_chance=feature_mutation_chance, test_split_amount=test_split_amount)\n",
    "    models = bagging_sa.run()\n",
    "    predictions = predict(X=X_test, models=models)\n",
    "    accuracy, correct_pred_amount, wrong_pred_amount = get_measures(predictions, y_test)\n",
    "    return accuracy, correct_pred_amount, wrong_pred_amount\n",
    "    \n",
    "def evaluate_decision_tree(X_train, y_train, X_test, y_test):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy, correct_pred_amount, wrong_pred_amount = get_measures(y_pred, y_test)\n",
    "    return accuracy, correct_pred_amount, wrong_pred_amount\n",
    "\n",
    "\n",
    "result = []\n",
    "print(f\"Start at {pd.Timestamp.now()}\")\n",
    "for dataset in datasets:\n",
    "    X, y = get_dataset(dataset)       \n",
    "    \n",
    "    random_indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(random_indices)\n",
    "    X = X[random_indices]\n",
    "    y = y[random_indices]\n",
    "    \n",
    "    sub_groups_X = np.array_split(np.array(X), k_cross)\n",
    "    sub_groups_y = np.array_split(np.array(y), k_cross) \n",
    "         \n",
    "    for n_tree in n_trees:\n",
    "        for k in range(k_cross):\n",
    "            X_train = np.concatenate(sub_groups_X[:k] + sub_groups_X[k+1:])\n",
    "            y_train = np.concatenate(sub_groups_y[:k] + sub_groups_y[k+1:])\n",
    "            X_test = sub_groups_X[k]\n",
    "            y_test = sub_groups_y[k]\n",
    "            \n",
    "            dt_acc, dt_correct, dt_wrong = evaluate_decision_tree(X_train, y_train, X_test, y_test)\n",
    "            bagging_acc, bagging_correct, bagging_wrong = evaluate_bagging(X_train, y_train, X_test, y_test, n_trees=n_tree)\n",
    "\n",
    "            pars = bagging_sa_params[dataset]\n",
    "            bagging_sa_acc, bagging_sa_correct, bagging_sa_wrong = evaluate_bagging_sa(X_train, y_train, X_test, y_test, n_trees=n_tree, params=pars)\n",
    "            \n",
    "            print(f\"    Dataset: {dataset}, n_trees: {n_tree}, k: {k+1}/{k_cross} >> DT: {dt_acc:.4f}, Bagging: {bagging_acc:.4f}, BaggingSA: {bagging_sa_acc:.4f}\")\n",
    "            \n",
    "            result.append([\n",
    "                dataset, n_tree, k+1, dt_acc, bagging_acc, bagging_sa_acc,\n",
    "                dt_correct, bagging_correct, bagging_sa_correct,\n",
    "                dt_wrong, bagging_wrong, bagging_sa_wrong\n",
    "            ])\n",
    "            \n",
    "            df = pd.DataFrame(result, columns=[\n",
    "                'dataset', 'nTrees', 'kCrossIndex', 'dtAccuracy', 'baggingAccuracy', 'baggingSAAccuracy',\n",
    "                'dtCorrectPred', 'baggingCorrectPred', 'baggingSACorrectPred',\n",
    "                'dtWrongPred', 'baggingWrongPred', 'baggingSAWrongPred'\n",
    "            ])\n",
    "            \n",
    "            df.to_csv(f'./../res/accuracy_comparison_{dataset}.csv', index=False)                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
