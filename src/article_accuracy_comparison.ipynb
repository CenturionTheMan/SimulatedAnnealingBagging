{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92a4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from raw_python.BaggingSA import BaggingSA\n",
    "from typing import Literal, Tuple\n",
    "import sklearn\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from raw_python.Bagging import create_models, create_bags, evaluate, predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ad331",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "k_cross = 5\n",
    "reps = 5\n",
    "n_trees = [10, 20, 30, 40, 50]\n",
    "datasets = ['digits', 'wine', 'breast_cancer', 'pima']\n",
    "\n",
    "bagging_sa_params = {\n",
    "    'wine' : {\n",
    "        'T0': 2,\n",
    "        'cooling_method': 'geometric',\n",
    "        'alpha': 0.995,\n",
    "        'max_iterations': 2000,\n",
    "        'feature_mutation_chance': 0.25,\n",
    "        'test_split_amount': 5       \n",
    "    },\n",
    "    'breast_cancer' : {\n",
    "        'T0': 2,\n",
    "        'cooling_method': 'geometric',\n",
    "        'alpha': 0.995,\n",
    "        'max_iterations': 2000,\n",
    "        'feature_mutation_chance': 0.25,\n",
    "        'test_split_amount': 5        \n",
    "    },\n",
    "    'pima' : {\n",
    "        'T0': 2,\n",
    "        'cooling_method': 'geometric',\n",
    "        'alpha': 0.995,\n",
    "        'max_iterations': 2000,\n",
    "        'feature_mutation_chance': 0.25,\n",
    "        'test_split_amount': 5         \n",
    "    },\n",
    "    \n",
    "    'digits' : {\n",
    "        'T0': 2,\n",
    "        'cooling_method': 'geometric',\n",
    "        'alpha': 0.995,\n",
    "        'max_iterations': 2000,\n",
    "        'feature_mutation_chance': 0.25,\n",
    "        'test_split_amount': 5         \n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7538a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if dataset_name == 'digits':\n",
    "        data = sklearn.datasets.load_digits()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "    elif dataset_name == 'wine':\n",
    "        data = sklearn.datasets.load_wine()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "    \n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = sklearn.datasets.load_breast_cancer()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "    elif dataset_name == 'pima':\n",
    "        data = pd.read_csv(\"./../datasets/pima.csv\")\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2025-04-24 19:20:44.049623\n",
      "    Dataset: digits, n_trees: 10, k: 1/5 >> DT: 0.850, Bagging: 0.944, RF: 0.958, BaggingCustom: 0.925, BaggingSA: 0.956\n",
      "    Dataset: digits, n_trees: 10, k: 2/5 >> DT: 0.872, Bagging: 0.942, RF: 0.956, BaggingCustom: 0.953, BaggingSA: 0.950\n",
      "    Dataset: digits, n_trees: 10, k: 3/5 >> DT: 0.827, Bagging: 0.911, RF: 0.942, BaggingCustom: 0.900, BaggingSA: 0.925\n",
      "    Dataset: digits, n_trees: 10, k: 4/5 >> DT: 0.861, Bagging: 0.911, RF: 0.953, BaggingCustom: 0.919, BaggingSA: 0.950\n",
      "    Dataset: digits, n_trees: 10, k: 5/5 >> DT: 0.861, Bagging: 0.933, RF: 0.939, BaggingCustom: 0.930, BaggingSA: 0.939\n",
      "    Dataset: digits, n_trees: 20, k: 1/5 >> DT: 0.861, Bagging: 0.944, RF: 0.964, BaggingCustom: 0.944, BaggingSA: 0.953\n",
      "    Dataset: digits, n_trees: 20, k: 2/5 >> DT: 0.875, Bagging: 0.956, RF: 0.978, BaggingCustom: 0.944, BaggingSA: 0.956\n",
      "    Dataset: digits, n_trees: 20, k: 3/5 >> DT: 0.836, Bagging: 0.925, RF: 0.950, BaggingCustom: 0.914, BaggingSA: 0.928\n",
      "    Dataset: digits, n_trees: 20, k: 4/5 >> DT: 0.877, Bagging: 0.919, RF: 0.969, BaggingCustom: 0.930, BaggingSA: 0.964\n",
      "    Dataset: digits, n_trees: 20, k: 5/5 >> DT: 0.850, Bagging: 0.942, RF: 0.964, BaggingCustom: 0.947, BaggingSA: 0.967\n",
      "    Dataset: digits, n_trees: 30, k: 1/5 >> DT: 0.850, Bagging: 0.958, RF: 0.964, BaggingCustom: 0.942, BaggingSA: 0.950\n",
      "    Dataset: digits, n_trees: 30, k: 2/5 >> DT: 0.894, Bagging: 0.964, RF: 0.983, BaggingCustom: 0.961, BaggingSA: 0.958\n",
      "    Dataset: digits, n_trees: 30, k: 3/5 >> DT: 0.841, Bagging: 0.919, RF: 0.958, BaggingCustom: 0.914, BaggingSA: 0.950\n",
      "    Dataset: digits, n_trees: 30, k: 4/5 >> DT: 0.869, Bagging: 0.930, RF: 0.972, BaggingCustom: 0.939, BaggingSA: 0.958\n",
      "    Dataset: digits, n_trees: 30, k: 5/5 >> DT: 0.858, Bagging: 0.947, RF: 0.972, BaggingCustom: 0.955, BaggingSA: 0.953\n",
      "    Dataset: digits, n_trees: 40, k: 1/5 >> DT: 0.858, Bagging: 0.956, RF: 0.967, BaggingCustom: 0.944, BaggingSA: 0.953\n",
      "    Dataset: digits, n_trees: 40, k: 2/5 >> DT: 0.872, Bagging: 0.964, RF: 0.981, BaggingCustom: 0.958, BaggingSA: 0.978\n",
      "    Dataset: digits, n_trees: 40, k: 3/5 >> DT: 0.844, Bagging: 0.930, RF: 0.964, BaggingCustom: 0.925, BaggingSA: 0.930\n",
      "    Dataset: digits, n_trees: 40, k: 4/5 >> DT: 0.861, Bagging: 0.930, RF: 0.978, BaggingCustom: 0.933, BaggingSA: 0.950\n",
      "    Dataset: digits, n_trees: 40, k: 5/5 >> DT: 0.861, Bagging: 0.944, RF: 0.975, BaggingCustom: 0.939, BaggingSA: 0.967\n",
      "    Dataset: digits, n_trees: 50, k: 1/5 >> DT: 0.856, Bagging: 0.956, RF: 0.972, BaggingCustom: 0.942, BaggingSA: 0.958\n",
      "    Dataset: digits, n_trees: 50, k: 2/5 >> DT: 0.881, Bagging: 0.958, RF: 0.981, BaggingCustom: 0.956, BaggingSA: 0.978\n",
      "    Dataset: digits, n_trees: 50, k: 3/5 >> DT: 0.844, Bagging: 0.930, RF: 0.964, BaggingCustom: 0.925, BaggingSA: 0.944\n",
      "    Dataset: digits, n_trees: 50, k: 4/5 >> DT: 0.861, Bagging: 0.939, RF: 0.986, BaggingCustom: 0.953, BaggingSA: 0.967\n",
      "    Dataset: digits, n_trees: 50, k: 5/5 >> DT: 0.864, Bagging: 0.955, RF: 0.978, BaggingCustom: 0.958, BaggingSA: 0.953\n",
      "    Dataset: digits, n_trees: 60, k: 1/5 >> DT: 0.861, Bagging: 0.956, RF: 0.972, BaggingCustom: 0.956, BaggingSA: 0.961\n",
      "    Dataset: digits, n_trees: 60, k: 2/5 >> DT: 0.878, Bagging: 0.961, RF: 0.986, BaggingCustom: 0.956, BaggingSA: 0.969\n",
      "    Dataset: digits, n_trees: 60, k: 3/5 >> DT: 0.833, Bagging: 0.930, RF: 0.967, BaggingCustom: 0.942, BaggingSA: 0.919\n",
      "    Dataset: digits, n_trees: 60, k: 4/5 >> DT: 0.866, Bagging: 0.933, RF: 0.989, BaggingCustom: 0.944, BaggingSA: 0.947\n",
      "    Dataset: digits, n_trees: 60, k: 5/5 >> DT: 0.861, Bagging: 0.947, RF: 0.975, BaggingCustom: 0.953, BaggingSA: 0.958\n",
      "    Dataset: digits, n_trees: 70, k: 1/5 >> DT: 0.856, Bagging: 0.950, RF: 0.972, BaggingCustom: 0.950, BaggingSA: 0.950\n",
      "    Dataset: digits, n_trees: 70, k: 2/5 >> DT: 0.881, Bagging: 0.956, RF: 0.989, BaggingCustom: 0.958, BaggingSA: 0.972\n",
      "    Dataset: digits, n_trees: 70, k: 3/5 >> DT: 0.838, Bagging: 0.933, RF: 0.961, BaggingCustom: 0.933, BaggingSA: 0.936\n",
      "    Dataset: digits, n_trees: 70, k: 4/5 >> DT: 0.861, Bagging: 0.936, RF: 0.986, BaggingCustom: 0.942, BaggingSA: 0.953\n",
      "    Dataset: digits, n_trees: 70, k: 5/5 >> DT: 0.852, Bagging: 0.953, RF: 0.981, BaggingCustom: 0.955, BaggingSA: 0.950\n",
      "    Dataset: digits, n_trees: 80, k: 1/5 >> DT: 0.847, Bagging: 0.950, RF: 0.972, BaggingCustom: 0.950, BaggingSA: 0.969\n",
      "    Dataset: digits, n_trees: 80, k: 2/5 >> DT: 0.875, Bagging: 0.958, RF: 0.986, BaggingCustom: 0.958, BaggingSA: 0.956\n",
      "    Dataset: digits, n_trees: 80, k: 3/5 >> DT: 0.844, Bagging: 0.930, RF: 0.961, BaggingCustom: 0.930, BaggingSA: 0.947\n",
      "    Dataset: digits, n_trees: 80, k: 4/5 >> DT: 0.866, Bagging: 0.936, RF: 0.986, BaggingCustom: 0.942, BaggingSA: 0.955\n",
      "    Dataset: digits, n_trees: 80, k: 5/5 >> DT: 0.864, Bagging: 0.953, RF: 0.983, BaggingCustom: 0.947, BaggingSA: 0.955\n",
      "    Dataset: digits, n_trees: 90, k: 1/5 >> DT: 0.844, Bagging: 0.947, RF: 0.972, BaggingCustom: 0.939, BaggingSA: 0.975\n",
      "    Dataset: digits, n_trees: 90, k: 2/5 >> DT: 0.878, Bagging: 0.958, RF: 0.981, BaggingCustom: 0.958, BaggingSA: 0.953\n",
      "    Dataset: digits, n_trees: 90, k: 3/5 >> DT: 0.844, Bagging: 0.936, RF: 0.958, BaggingCustom: 0.933, BaggingSA: 0.942\n",
      "    Dataset: digits, n_trees: 90, k: 4/5 >> DT: 0.880, Bagging: 0.936, RF: 0.983, BaggingCustom: 0.950, BaggingSA: 0.958\n",
      "    Dataset: digits, n_trees: 90, k: 5/5 >> DT: 0.872, Bagging: 0.953, RF: 0.983, BaggingCustom: 0.958, BaggingSA: 0.955\n",
      "    Dataset: digits, n_trees: 100, k: 1/5 >> DT: 0.847, Bagging: 0.942, RF: 0.972, BaggingCustom: 0.942, BaggingSA: 0.964\n",
      "    Dataset: digits, n_trees: 100, k: 2/5 >> DT: 0.886, Bagging: 0.961, RF: 0.981, BaggingCustom: 0.956, BaggingSA: 0.964\n",
      "    Dataset: digits, n_trees: 100, k: 3/5 >> DT: 0.838, Bagging: 0.933, RF: 0.961, BaggingCustom: 0.928, BaggingSA: 0.942\n",
      "    Dataset: digits, n_trees: 100, k: 4/5 >> DT: 0.852, Bagging: 0.939, RF: 0.981, BaggingCustom: 0.936, BaggingSA: 0.969\n",
      "    Dataset: digits, n_trees: 100, k: 5/5 >> DT: 0.861, Bagging: 0.955, RF: 0.983, BaggingCustom: 0.950, BaggingSA: 0.955\n",
      "    Dataset: wine, n_trees: 10, k: 1/5 >> DT: 0.889, Bagging: 0.889, RF: 0.917, BaggingCustom: 0.889, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 10, k: 2/5 >> DT: 1.000, Bagging: 0.944, RF: 0.972, BaggingCustom: 1.000, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 10, k: 3/5 >> DT: 0.917, Bagging: 0.917, RF: 0.972, BaggingCustom: 0.889, BaggingSA: 0.917\n",
      "    Dataset: wine, n_trees: 10, k: 4/5 >> DT: 0.943, Bagging: 0.943, RF: 1.000, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 10, k: 5/5 >> DT: 0.886, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.886, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 20, k: 1/5 >> DT: 0.833, Bagging: 0.944, RF: 0.944, BaggingCustom: 0.944, BaggingSA: 0.972\n",
      "    Dataset: wine, n_trees: 20, k: 2/5 >> DT: 0.972, Bagging: 1.000, RF: 1.000, BaggingCustom: 0.944, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 20, k: 3/5 >> DT: 0.889, Bagging: 0.944, RF: 0.972, BaggingCustom: 0.944, BaggingSA: 0.944\n",
      "    Dataset: wine, n_trees: 20, k: 4/5 >> DT: 0.886, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 20, k: 5/5 >> DT: 0.886, Bagging: 0.914, RF: 0.971, BaggingCustom: 0.943, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 30, k: 1/5 >> DT: 0.833, Bagging: 0.889, RF: 0.944, BaggingCustom: 0.889, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 30, k: 2/5 >> DT: 0.972, Bagging: 1.000, RF: 1.000, BaggingCustom: 1.000, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 30, k: 3/5 >> DT: 0.917, Bagging: 0.944, RF: 0.972, BaggingCustom: 0.889, BaggingSA: 0.944\n",
      "    Dataset: wine, n_trees: 30, k: 4/5 >> DT: 0.886, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 30, k: 5/5 >> DT: 0.886, Bagging: 0.943, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 40, k: 1/5 >> DT: 0.889, Bagging: 0.889, RF: 0.944, BaggingCustom: 0.889, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 40, k: 2/5 >> DT: 0.972, Bagging: 1.000, RF: 1.000, BaggingCustom: 0.972, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 40, k: 3/5 >> DT: 0.917, Bagging: 0.944, RF: 0.972, BaggingCustom: 0.944, BaggingSA: 0.917\n",
      "    Dataset: wine, n_trees: 40, k: 4/5 >> DT: 0.914, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 40, k: 5/5 >> DT: 0.886, Bagging: 0.943, RF: 0.971, BaggingCustom: 0.943, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 50, k: 1/5 >> DT: 0.861, Bagging: 0.889, RF: 0.944, BaggingCustom: 0.917, BaggingSA: 0.917\n",
      "    Dataset: wine, n_trees: 50, k: 2/5 >> DT: 1.000, Bagging: 1.000, RF: 1.000, BaggingCustom: 0.972, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 50, k: 3/5 >> DT: 0.889, Bagging: 0.944, RF: 0.972, BaggingCustom: 0.944, BaggingSA: 0.944\n",
      "    Dataset: wine, n_trees: 50, k: 4/5 >> DT: 0.943, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 50, k: 5/5 >> DT: 0.857, Bagging: 0.943, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 60, k: 1/5 >> DT: 0.861, Bagging: 0.889, RF: 0.944, BaggingCustom: 0.917, BaggingSA: 0.889\n",
      "    Dataset: wine, n_trees: 60, k: 2/5 >> DT: 1.000, Bagging: 1.000, RF: 1.000, BaggingCustom: 1.000, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 60, k: 3/5 >> DT: 0.917, Bagging: 0.944, RF: 0.972, BaggingCustom: 0.917, BaggingSA: 0.917\n",
      "    Dataset: wine, n_trees: 60, k: 4/5 >> DT: 0.829, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 60, k: 5/5 >> DT: 0.886, Bagging: 0.943, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 70, k: 1/5 >> DT: 0.861, Bagging: 0.917, RF: 0.972, BaggingCustom: 0.889, BaggingSA: 0.972\n",
      "    Dataset: wine, n_trees: 70, k: 2/5 >> DT: 0.972, Bagging: 1.000, RF: 1.000, BaggingCustom: 0.944, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 70, k: 3/5 >> DT: 0.861, Bagging: 0.944, RF: 0.944, BaggingCustom: 0.917, BaggingSA: 0.944\n",
      "    Dataset: wine, n_trees: 70, k: 4/5 >> DT: 0.886, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 70, k: 5/5 >> DT: 0.886, Bagging: 0.943, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 80, k: 1/5 >> DT: 0.806, Bagging: 0.889, RF: 1.000, BaggingCustom: 0.889, BaggingSA: 0.889\n",
      "    Dataset: wine, n_trees: 80, k: 2/5 >> DT: 0.972, Bagging: 1.000, RF: 1.000, BaggingCustom: 0.944, BaggingSA: 0.972\n",
      "    Dataset: wine, n_trees: 80, k: 3/5 >> DT: 0.889, Bagging: 0.944, RF: 0.944, BaggingCustom: 0.917, BaggingSA: 0.917\n",
      "    Dataset: wine, n_trees: 80, k: 4/5 >> DT: 0.914, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 80, k: 5/5 >> DT: 0.886, Bagging: 0.943, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 90, k: 1/5 >> DT: 0.806, Bagging: 0.889, RF: 1.000, BaggingCustom: 0.972, BaggingSA: 0.917\n",
      "    Dataset: wine, n_trees: 90, k: 2/5 >> DT: 0.972, Bagging: 1.000, RF: 1.000, BaggingCustom: 0.944, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 90, k: 3/5 >> DT: 0.917, Bagging: 0.944, RF: 0.944, BaggingCustom: 0.944, BaggingSA: 0.944\n",
      "    Dataset: wine, n_trees: 90, k: 4/5 >> DT: 0.829, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 90, k: 5/5 >> DT: 0.886, Bagging: 0.943, RF: 0.971, BaggingCustom: 0.943, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 100, k: 1/5 >> DT: 0.833, Bagging: 0.889, RF: 1.000, BaggingCustom: 0.944, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 100, k: 2/5 >> DT: 0.972, Bagging: 1.000, RF: 1.000, BaggingCustom: 0.944, BaggingSA: 1.000\n",
      "    Dataset: wine, n_trees: 100, k: 3/5 >> DT: 0.861, Bagging: 0.944, RF: 0.944, BaggingCustom: 0.944, BaggingSA: 0.944\n",
      "    Dataset: wine, n_trees: 100, k: 4/5 >> DT: 0.886, Bagging: 0.971, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: wine, n_trees: 100, k: 5/5 >> DT: 0.886, Bagging: 0.943, RF: 0.971, BaggingCustom: 0.971, BaggingSA: 0.971\n",
      "    Dataset: breast_cancer, n_trees: 10, k: 1/5 >> DT: 0.930, Bagging: 0.947, RF: 0.912, BaggingCustom: 0.921, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 10, k: 2/5 >> DT: 0.982, Bagging: 0.982, RF: 0.982, BaggingCustom: 0.982, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 10, k: 3/5 >> DT: 0.842, Bagging: 0.974, RF: 0.982, BaggingCustom: 0.939, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 10, k: 4/5 >> DT: 0.939, Bagging: 0.930, RF: 0.921, BaggingCustom: 0.939, BaggingSA: 0.930\n",
      "    Dataset: breast_cancer, n_trees: 10, k: 5/5 >> DT: 0.858, Bagging: 0.965, RF: 0.947, BaggingCustom: 0.885, BaggingSA: 0.929\n",
      "    Dataset: breast_cancer, n_trees: 20, k: 1/5 >> DT: 0.921, Bagging: 0.947, RF: 0.939, BaggingCustom: 0.930, BaggingSA: 0.956\n",
      "    Dataset: breast_cancer, n_trees: 20, k: 2/5 >> DT: 0.974, Bagging: 0.991, RF: 0.974, BaggingCustom: 0.991, BaggingSA: 1.000\n",
      "    Dataset: breast_cancer, n_trees: 20, k: 3/5 >> DT: 0.851, Bagging: 0.982, RF: 0.982, BaggingCustom: 0.982, BaggingSA: 0.982\n",
      "    Dataset: breast_cancer, n_trees: 20, k: 4/5 >> DT: 0.930, Bagging: 0.947, RF: 0.930, BaggingCustom: 0.947, BaggingSA: 0.939\n",
      "    Dataset: breast_cancer, n_trees: 20, k: 5/5 >> DT: 0.885, Bagging: 0.965, RF: 0.938, BaggingCustom: 0.920, BaggingSA: 0.894\n",
      "    Dataset: breast_cancer, n_trees: 30, k: 1/5 >> DT: 0.939, Bagging: 0.947, RF: 0.956, BaggingCustom: 0.947, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 30, k: 2/5 >> DT: 0.991, Bagging: 1.000, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 1.000\n",
      "    Dataset: breast_cancer, n_trees: 30, k: 3/5 >> DT: 0.860, Bagging: 0.982, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 30, k: 4/5 >> DT: 0.930, Bagging: 0.939, RF: 0.930, BaggingCustom: 0.930, BaggingSA: 0.930\n",
      "    Dataset: breast_cancer, n_trees: 30, k: 5/5 >> DT: 0.894, Bagging: 0.929, RF: 0.938, BaggingCustom: 0.938, BaggingSA: 0.920\n",
      "    Dataset: breast_cancer, n_trees: 40, k: 1/5 >> DT: 0.930, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.939, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 40, k: 2/5 >> DT: 0.974, Bagging: 1.000, RF: 0.991, BaggingCustom: 1.000, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 40, k: 3/5 >> DT: 0.895, Bagging: 0.991, RF: 0.991, BaggingCustom: 0.982, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 40, k: 4/5 >> DT: 0.939, Bagging: 0.939, RF: 0.939, BaggingCustom: 0.921, BaggingSA: 0.939\n",
      "    Dataset: breast_cancer, n_trees: 40, k: 5/5 >> DT: 0.867, Bagging: 0.929, RF: 0.938, BaggingCustom: 0.903, BaggingSA: 0.885\n",
      "    Dataset: breast_cancer, n_trees: 50, k: 1/5 >> DT: 0.912, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.930, BaggingSA: 0.930\n",
      "    Dataset: breast_cancer, n_trees: 50, k: 2/5 >> DT: 0.965, Bagging: 1.000, RF: 0.991, BaggingCustom: 1.000, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 50, k: 3/5 >> DT: 0.912, Bagging: 0.991, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 50, k: 4/5 >> DT: 0.939, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.939, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 50, k: 5/5 >> DT: 0.876, Bagging: 0.929, RF: 0.938, BaggingCustom: 0.912, BaggingSA: 0.903\n",
      "    Dataset: breast_cancer, n_trees: 60, k: 1/5 >> DT: 0.904, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.939, BaggingSA: 0.956\n",
      "    Dataset: breast_cancer, n_trees: 60, k: 2/5 >> DT: 0.982, Bagging: 1.000, RF: 0.991, BaggingCustom: 1.000, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 60, k: 3/5 >> DT: 0.868, Bagging: 0.991, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 0.965\n",
      "    Dataset: breast_cancer, n_trees: 60, k: 4/5 >> DT: 0.930, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.930, BaggingSA: 0.939\n",
      "    Dataset: breast_cancer, n_trees: 60, k: 5/5 >> DT: 0.894, Bagging: 0.929, RF: 0.938, BaggingCustom: 0.912, BaggingSA: 0.885\n",
      "    Dataset: breast_cancer, n_trees: 70, k: 1/5 >> DT: 0.904, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.939, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 70, k: 2/5 >> DT: 0.991, Bagging: 1.000, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 70, k: 3/5 >> DT: 0.851, Bagging: 0.991, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 0.974\n",
      "    Dataset: breast_cancer, n_trees: 70, k: 4/5 >> DT: 0.939, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.947, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 70, k: 5/5 >> DT: 0.876, Bagging: 0.929, RF: 0.938, BaggingCustom: 0.903, BaggingSA: 0.912\n",
      "    Dataset: breast_cancer, n_trees: 80, k: 1/5 >> DT: 0.921, Bagging: 0.947, RF: 0.939, BaggingCustom: 0.939, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 80, k: 2/5 >> DT: 0.982, Bagging: 1.000, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 1.000\n",
      "    Dataset: breast_cancer, n_trees: 80, k: 3/5 >> DT: 0.851, Bagging: 0.991, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 0.974\n",
      "    Dataset: breast_cancer, n_trees: 80, k: 4/5 >> DT: 0.930, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.947, BaggingSA: 0.947\n",
      "    Dataset: breast_cancer, n_trees: 80, k: 5/5 >> DT: 0.885, Bagging: 0.929, RF: 0.929, BaggingCustom: 0.912, BaggingSA: 0.885\n",
      "    Dataset: breast_cancer, n_trees: 90, k: 1/5 >> DT: 0.912, Bagging: 0.947, RF: 0.939, BaggingCustom: 0.939, BaggingSA: 0.939\n",
      "    Dataset: breast_cancer, n_trees: 90, k: 2/5 >> DT: 0.991, Bagging: 1.000, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 1.000\n",
      "    Dataset: breast_cancer, n_trees: 90, k: 3/5 >> DT: 0.877, Bagging: 0.991, RF: 0.991, BaggingCustom: 0.982, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 90, k: 4/5 >> DT: 0.921, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.939, BaggingSA: 0.930\n",
      "    Dataset: breast_cancer, n_trees: 90, k: 5/5 >> DT: 0.867, Bagging: 0.920, RF: 0.929, BaggingCustom: 0.920, BaggingSA: 0.903\n",
      "    Dataset: breast_cancer, n_trees: 100, k: 1/5 >> DT: 0.921, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.947, BaggingSA: 0.956\n",
      "    Dataset: breast_cancer, n_trees: 100, k: 2/5 >> DT: 0.956, Bagging: 0.991, RF: 0.991, BaggingCustom: 1.000, BaggingSA: 0.991\n",
      "    Dataset: breast_cancer, n_trees: 100, k: 3/5 >> DT: 0.895, Bagging: 0.991, RF: 0.991, BaggingCustom: 0.991, BaggingSA: 0.982\n",
      "    Dataset: breast_cancer, n_trees: 100, k: 4/5 >> DT: 0.921, Bagging: 0.947, RF: 0.947, BaggingCustom: 0.947, BaggingSA: 0.930\n",
      "    Dataset: breast_cancer, n_trees: 100, k: 5/5 >> DT: 0.858, Bagging: 0.929, RF: 0.920, BaggingCustom: 0.912, BaggingSA: 0.929\n",
      "    Dataset: pima, n_trees: 10, k: 1/5 >> DT: 0.701, Bagging: 0.708, RF: 0.708, BaggingCustom: 0.708, BaggingSA: 0.721\n",
      "    Dataset: pima, n_trees: 10, k: 2/5 >> DT: 0.695, Bagging: 0.825, RF: 0.773, BaggingCustom: 0.747, BaggingSA: 0.779\n",
      "    Dataset: pima, n_trees: 10, k: 3/5 >> DT: 0.610, Bagging: 0.701, RF: 0.721, BaggingCustom: 0.695, BaggingSA: 0.688\n",
      "    Dataset: pima, n_trees: 10, k: 4/5 >> DT: 0.693, Bagging: 0.765, RF: 0.765, BaggingCustom: 0.745, BaggingSA: 0.771\n",
      "    Dataset: pima, n_trees: 10, k: 5/5 >> DT: 0.647, Bagging: 0.739, RF: 0.765, BaggingCustom: 0.725, BaggingSA: 0.719\n",
      "    Dataset: pima, n_trees: 20, k: 1/5 >> DT: 0.721, Bagging: 0.682, RF: 0.740, BaggingCustom: 0.760, BaggingSA: 0.695\n",
      "    Dataset: pima, n_trees: 20, k: 2/5 >> DT: 0.708, Bagging: 0.831, RF: 0.805, BaggingCustom: 0.766, BaggingSA: 0.779\n",
      "    Dataset: pima, n_trees: 20, k: 3/5 >> DT: 0.688, Bagging: 0.714, RF: 0.721, BaggingCustom: 0.708, BaggingSA: 0.753\n",
      "    Dataset: pima, n_trees: 20, k: 4/5 >> DT: 0.693, Bagging: 0.771, RF: 0.784, BaggingCustom: 0.745, BaggingSA: 0.706\n",
      "    Dataset: pima, n_trees: 20, k: 5/5 >> DT: 0.680, Bagging: 0.725, RF: 0.739, BaggingCustom: 0.739, BaggingSA: 0.706\n",
      "    Dataset: pima, n_trees: 30, k: 1/5 >> DT: 0.688, Bagging: 0.727, RF: 0.727, BaggingCustom: 0.760, BaggingSA: 0.721\n",
      "    Dataset: pima, n_trees: 30, k: 2/5 >> DT: 0.695, Bagging: 0.805, RF: 0.792, BaggingCustom: 0.779, BaggingSA: 0.799\n",
      "    Dataset: pima, n_trees: 30, k: 3/5 >> DT: 0.649, Bagging: 0.727, RF: 0.701, BaggingCustom: 0.714, BaggingSA: 0.714\n",
      "    Dataset: pima, n_trees: 30, k: 4/5 >> DT: 0.693, Bagging: 0.758, RF: 0.810, BaggingCustom: 0.765, BaggingSA: 0.791\n",
      "    Dataset: pima, n_trees: 30, k: 5/5 >> DT: 0.680, Bagging: 0.725, RF: 0.752, BaggingCustom: 0.752, BaggingSA: 0.732\n",
      "    Dataset: pima, n_trees: 40, k: 1/5 >> DT: 0.727, Bagging: 0.753, RF: 0.727, BaggingCustom: 0.760, BaggingSA: 0.727\n",
      "    Dataset: pima, n_trees: 40, k: 2/5 >> DT: 0.708, Bagging: 0.799, RF: 0.786, BaggingCustom: 0.812, BaggingSA: 0.766\n",
      "    Dataset: pima, n_trees: 40, k: 3/5 >> DT: 0.617, Bagging: 0.708, RF: 0.708, BaggingCustom: 0.714, BaggingSA: 0.714\n",
      "    Dataset: pima, n_trees: 40, k: 4/5 >> DT: 0.686, Bagging: 0.791, RF: 0.797, BaggingCustom: 0.765, BaggingSA: 0.771\n",
      "    Dataset: pima, n_trees: 40, k: 5/5 >> DT: 0.647, Bagging: 0.725, RF: 0.758, BaggingCustom: 0.745, BaggingSA: 0.732\n",
      "    Dataset: pima, n_trees: 50, k: 1/5 >> DT: 0.701, Bagging: 0.766, RF: 0.747, BaggingCustom: 0.760, BaggingSA: 0.714\n",
      "    Dataset: pima, n_trees: 50, k: 2/5 >> DT: 0.701, Bagging: 0.805, RF: 0.786, BaggingCustom: 0.805, BaggingSA: 0.812\n",
      "    Dataset: pima, n_trees: 50, k: 3/5 >> DT: 0.617, Bagging: 0.721, RF: 0.714, BaggingCustom: 0.701, BaggingSA: 0.708\n",
      "    Dataset: pima, n_trees: 50, k: 4/5 >> DT: 0.686, Bagging: 0.784, RF: 0.784, BaggingCustom: 0.771, BaggingSA: 0.745\n",
      "    Dataset: pima, n_trees: 50, k: 5/5 >> DT: 0.686, Bagging: 0.732, RF: 0.752, BaggingCustom: 0.725, BaggingSA: 0.732\n",
      "    Dataset: pima, n_trees: 60, k: 1/5 >> DT: 0.708, Bagging: 0.734, RF: 0.734, BaggingCustom: 0.773, BaggingSA: 0.708\n",
      "    Dataset: pima, n_trees: 60, k: 2/5 >> DT: 0.701, Bagging: 0.805, RF: 0.792, BaggingCustom: 0.779, BaggingSA: 0.760\n",
      "    Dataset: pima, n_trees: 60, k: 3/5 >> DT: 0.623, Bagging: 0.721, RF: 0.701, BaggingCustom: 0.708, BaggingSA: 0.727\n",
      "    Dataset: pima, n_trees: 60, k: 4/5 >> DT: 0.719, Bagging: 0.778, RF: 0.791, BaggingCustom: 0.771, BaggingSA: 0.791\n",
      "    Dataset: pima, n_trees: 60, k: 5/5 >> DT: 0.647, Bagging: 0.725, RF: 0.758, BaggingCustom: 0.725, BaggingSA: 0.765\n",
      "    Dataset: pima, n_trees: 70, k: 1/5 >> DT: 0.701, Bagging: 0.760, RF: 0.766, BaggingCustom: 0.792, BaggingSA: 0.747\n",
      "    Dataset: pima, n_trees: 70, k: 2/5 >> DT: 0.714, Bagging: 0.812, RF: 0.786, BaggingCustom: 0.825, BaggingSA: 0.779\n",
      "    Dataset: pima, n_trees: 70, k: 3/5 >> DT: 0.675, Bagging: 0.727, RF: 0.708, BaggingCustom: 0.695, BaggingSA: 0.721\n",
      "    Dataset: pima, n_trees: 70, k: 4/5 >> DT: 0.712, Bagging: 0.784, RF: 0.784, BaggingCustom: 0.778, BaggingSA: 0.752\n",
      "    Dataset: pima, n_trees: 70, k: 5/5 >> DT: 0.660, Bagging: 0.719, RF: 0.752, BaggingCustom: 0.752, BaggingSA: 0.745\n",
      "    Dataset: pima, n_trees: 80, k: 1/5 >> DT: 0.721, Bagging: 0.753, RF: 0.760, BaggingCustom: 0.760, BaggingSA: 0.721\n",
      "    Dataset: pima, n_trees: 80, k: 2/5 >> DT: 0.714, Bagging: 0.818, RF: 0.786, BaggingCustom: 0.792, BaggingSA: 0.766\n",
      "    Dataset: pima, n_trees: 80, k: 3/5 >> DT: 0.643, Bagging: 0.721, RF: 0.708, BaggingCustom: 0.695, BaggingSA: 0.727\n",
      "    Dataset: pima, n_trees: 80, k: 4/5 >> DT: 0.699, Bagging: 0.797, RF: 0.784, BaggingCustom: 0.758, BaggingSA: 0.752\n",
      "    Dataset: pima, n_trees: 80, k: 5/5 >> DT: 0.660, Bagging: 0.725, RF: 0.765, BaggingCustom: 0.758, BaggingSA: 0.745\n",
      "    Dataset: pima, n_trees: 90, k: 1/5 >> DT: 0.721, Bagging: 0.760, RF: 0.760, BaggingCustom: 0.760, BaggingSA: 0.740\n",
      "    Dataset: pima, n_trees: 90, k: 2/5 >> DT: 0.721, Bagging: 0.818, RF: 0.792, BaggingCustom: 0.799, BaggingSA: 0.773\n",
      "    Dataset: pima, n_trees: 90, k: 3/5 >> DT: 0.675, Bagging: 0.721, RF: 0.708, BaggingCustom: 0.701, BaggingSA: 0.721\n",
      "    Dataset: pima, n_trees: 90, k: 4/5 >> DT: 0.699, Bagging: 0.804, RF: 0.784, BaggingCustom: 0.765, BaggingSA: 0.765\n",
      "    Dataset: pima, n_trees: 90, k: 5/5 >> DT: 0.647, Bagging: 0.732, RF: 0.765, BaggingCustom: 0.752, BaggingSA: 0.778\n",
      "    Dataset: pima, n_trees: 100, k: 1/5 >> DT: 0.708, Bagging: 0.760, RF: 0.740, BaggingCustom: 0.747, BaggingSA: 0.721\n",
      "    Dataset: pima, n_trees: 100, k: 2/5 >> DT: 0.708, Bagging: 0.825, RF: 0.786, BaggingCustom: 0.825, BaggingSA: 0.805\n",
      "    Dataset: pima, n_trees: 100, k: 3/5 >> DT: 0.630, Bagging: 0.727, RF: 0.701, BaggingCustom: 0.701, BaggingSA: 0.721\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m rf_acc = evaluate_rf(X_train, y_train, X_test, y_test, n_trees=n_tree)\n\u001b[32m     67\u001b[39m bagging_custom_acc = evaluate_bagging_custom(X_train, y_train, X_test, y_test, n_trees=n_tree)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m bagging_sa_acc = \u001b[43mevaluate_bagging_sa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trees\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, n_trees: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tree\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk_cross\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m >> DT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Bagging: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbagging_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, RF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrf_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, BaggingCustom: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbagging_custom_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, BaggingSA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbagging_sa_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m result.append([\n\u001b[32m     73\u001b[39m     dataset, n_tree, k+\u001b[32m1\u001b[39m, dt_acc, bagging_acc, rf_acc, bagging_custom_acc, bagging_sa_acc\n\u001b[32m     74\u001b[39m ])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mevaluate_bagging_sa\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, n_trees, params)\u001b[39m\n\u001b[32m     21\u001b[39m test_split_amount = params[\u001b[33m'\u001b[39m\u001b[33mtest_split_amount\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     22\u001b[39m bagging_sa = BaggingSA(X=X_train, y=y_train,\n\u001b[32m     23\u001b[39m                         T0=T0, cooling_method=cooling_method, alpha=alpha, max_iterations=max_iterations, n_trees=n_trees,\n\u001b[32m     24\u001b[39m                         feature_mutation_chance=feature_mutation_chance, test_split_amount=test_split_amount)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m models = \u001b[43mbagging_sa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m accuracy = evaluate(X=X_test, y=y_test, models=models)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/src/raw_python/BaggingSA.py:151\u001b[39m, in \u001b[36mBaggingSA.run\u001b[39m\u001b[34m(self, X_for_test, y_for_test, monitor_fun, get_fitness)\u001b[39m\n\u001b[32m    149\u001b[39m new_bags = \u001b[38;5;28mself\u001b[39m.get_neighbors(bags)\n\u001b[32m    150\u001b[39m models = create_models(new_bags)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m new_fitness = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m accuracy = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_for_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y_for_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/src/raw_python/BaggingSA.py:60\u001b[39m, in \u001b[36mBaggingSA.calculate_fitness\u001b[39m\u001b[34m(self, models)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_fitness\u001b[39m(\u001b[38;5;28mself\u001b[39m, models: List[BaggingModel]) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m     58\u001b[39m     sub_groups_X_test, sub_groups_y_test = \u001b[38;5;28mself\u001b[39m.get_validate_sets()\n\u001b[32m     59\u001b[39m     accuracies = [\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_groups_X_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_groups_y_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.test_split_amount)\n\u001b[32m     62\u001b[39m     ]\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(accuracies)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/src/raw_python/Bagging.py:110\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(X, y, models)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(X, y, models: List[BaggingModel]) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     predictions = \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     accuracy = accuracy_score(y, predictions)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/src/raw_python/Bagging.py:104\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(X, models)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(X, models: List[BaggingModel]) -> np.ndarray:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     predictions = [ \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_X_by_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models ]\n\u001b[32m    105\u001b[39m     predictions = np.array(predictions)\n\u001b[32m    106\u001b[39m     final_predictions = [np.bincount(pred).argmax() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions.T]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/.venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:530\u001b[39m, in \u001b[36mBaseDecisionTree.predict\u001b[39m\u001b[34m(self, X, check_input)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[32m    508\u001b[39m \n\u001b[32m    509\u001b[39m \u001b[33;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    527\u001b[39m \u001b[33;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m proba = \u001b[38;5;28mself\u001b[39m.tree_.predict(X)\n\u001b[32m    532\u001b[39m n_samples = X.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/.venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:489\u001b[39m, in \u001b[36mBaseDecisionTree._validate_X_predict\u001b[39m\u001b[34m(self, X, check_input)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    488\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    498\u001b[39m     X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc\n\u001b[32m    499\u001b[39m ):\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:116\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(over=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     first_pass_isfinite = \u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m(xp.sum(X))\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HDD/Univerity/SimulatedAnnealingBagging/.venv/lib/python3.12/site-packages/sklearn/utils/_array_api.py:381\u001b[39m, in \u001b[36m_NumPyAPIWrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# Data types in spec\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# https://data-apis.org/array-api/latest/API_specification/data_types.html\u001b[39;00m\n\u001b[32m    363\u001b[39m _DTYPES = {\n\u001b[32m    364\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mint8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    365\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mint16\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomplex128\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    379\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m    382\u001b[39m     attr = \u001b[38;5;28mgetattr\u001b[39m(numpy, name)\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# Support device kwargs and make sure they are on the CPU\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def evaluate_rf(X_train, y_train, X_test, y_test, n_trees: int) -> float:\n",
    "    model = RandomForestClassifier(n_estimators=n_trees, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_bagging_custom(X_train, y_train, X_test, y_test, n_trees: int) -> float:\n",
    "    bags = create_bags(X_train, y_train, bags_amount=n_trees)\n",
    "    models = create_models(bags=bags)\n",
    "    accuracy = evaluate(X=X_test, y=y_test, models=models)\n",
    "    return accuracy\n",
    "  \n",
    "    \n",
    "def evaluate_bagging_sa(X_train, y_train, X_test, y_test, n_trees: int, params: dict) -> float:\n",
    "    T0 = params['T0']\n",
    "    cooling_method = params['cooling_method']\n",
    "    alpha = params['alpha']\n",
    "    max_iterations = params['max_iterations']\n",
    "    feature_mutation_chance = params['feature_mutation_chance']\n",
    "    test_split_amount = params['test_split_amount']\n",
    "    bagging_sa = BaggingSA(X=X_train, y=y_train,\n",
    "                            T0=T0, cooling_method=cooling_method, alpha=alpha, max_iterations=max_iterations, n_trees=n_trees,\n",
    "                            feature_mutation_chance=feature_mutation_chance, test_split_amount=test_split_amount)\n",
    "    models = bagging_sa.run()\n",
    "    accuracy = evaluate(X=X_test, y=y_test, models=models)\n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "def evaluate_decision_tree(X_train, y_train, X_test, y_test):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_bagging(X_train, y_train, X_test, y_test, n_trees: int) -> float:\n",
    "    model = BaggingClassifier(n_estimators=n_trees, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    return accuracy    \n",
    "\n",
    "print(f\"Start at {pd.Timestamp.now()}\")\n",
    "for dataset in datasets:\n",
    "    result = []\n",
    "    X, y = get_dataset(dataset)       \n",
    "    \n",
    "    random_indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(random_indices)\n",
    "    X = X[random_indices]\n",
    "    y = y[random_indices]\n",
    "    \n",
    "    sub_groups_X = np.array_split(np.array(X), k_cross)\n",
    "    sub_groups_y = np.array_split(np.array(y), k_cross) \n",
    "         \n",
    "    for n_tree in n_trees:\n",
    "        for rep in range(rep):\n",
    "            for k in range(k_cross):\n",
    "                X_train = np.concatenate(sub_groups_X[:k] + sub_groups_X[k+1:])\n",
    "                y_train = np.concatenate(sub_groups_y[:k] + sub_groups_y[k+1:])\n",
    "                X_test = sub_groups_X[k]\n",
    "                y_test = sub_groups_y[k]\n",
    "                pars = bagging_sa_params[dataset]\n",
    "                \n",
    "                dt_acc= evaluate_decision_tree(X_train, y_train, X_test, y_test)\n",
    "                bagging_acc = evaluate_bagging(X_train, y_train, X_test, y_test, n_trees=n_tree)\n",
    "                rf_acc = evaluate_rf(X_train, y_train, X_test, y_test, n_trees=n_tree)\n",
    "                bagging_custom_acc = evaluate_bagging_custom(X_train, y_train, X_test, y_test, n_trees=n_tree)\n",
    "                bagging_sa_acc = evaluate_bagging_sa(X_train, y_train, X_test, y_test, n_trees=n_tree, params=pars)\n",
    "                \n",
    "                print(f\"    Dataset: {dataset}, n_trees: {n_tree}, rep: {rep}, k: {k+1}/{k_cross} >> DT: {dt_acc:.3f}, Bagging: {bagging_acc:.3f}, RF: {rf_acc:.3f}, BaggingCustom: {bagging_custom_acc:.3f}, BaggingSA: {bagging_sa_acc:.3f}\")\n",
    "                \n",
    "                result.append([\n",
    "                    dataset, n_tree, rep, k+1, dt_acc, bagging_acc, rf_acc, bagging_custom_acc, bagging_sa_acc\n",
    "                ])\n",
    "                \n",
    "                df = pd.DataFrame(result, columns=[\n",
    "                    \"Dataset\",\n",
    "                    \"nTrees\",\n",
    "                    \"Rep\",\n",
    "                    \"K\",\n",
    "                    \"DT\",\n",
    "                    \"Bagging\",\n",
    "                    \"RF\",\n",
    "                    \"BaggingCustom\",\n",
    "                    \"BaggingSA\"\n",
    "                ])\n",
    "                \n",
    "                df.to_csv(f'./../res/accuracy_comparison_{dataset}.csv', index=False)                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
